{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "import os\n",
    "import sys\n",
    "cur_dir = os.path.dirname(os.path.abspath(\"__file__\"))  # Gets the current notebook directory\n",
    "src_dir = os.path.join(cur_dir, '../')  # Constructs the path to the 'src' directory\n",
    "# Add the 'src' directory to sys.path\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "from src.constant import *\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.MyDataset import MyDataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lookback = 20\n",
    "dir = '../data/PandasData/Sampled/'\n",
    "ds = MyDataset(lookback=lookback)\n",
    "train_batch_size = 4\n",
    "test_batch_size = 16\n",
    "\n",
    "def process_data(df_dir : str, target_freq : int = 10):\n",
    "    df: pd.DataFrame = pd.read_pickle(df_dir)\n",
    "    df.dropna(inplace=True, how='any')\n",
    "    f_per_sec = df.groupby('TimestampID').count().mean().mean()\n",
    "    if f_per_sec < target_freq:\n",
    "        raise ValueError('The frequency of the data is lower than the target frequency')\n",
    "    elif int(f_per_sec) == target_freq:\n",
    "        pass\n",
    "    else:\n",
    "        resample_ratio = int(f_per_sec/target_freq)\n",
    "        df = df.iloc[::resample_ratio, :]\n",
    "    # # for origin\n",
    "    for drop_column in ['Confidence', \n",
    "                          'Timestamp', 'TimestampID', \n",
    "                          'DatapointID', 'PID', 'SCN', 'U_X', 'U_Y', 'U_Z', \n",
    "                          'AGV_Z', 'User_Z', 'GazeOrigin_Z', 'User_Pitch', 'User_Yaw', 'User_Roll', \n",
    "                          'EyeTarget']:\n",
    "        df = df.drop(columns=[drop_column], errors='ignore')\n",
    "\n",
    "    target_columns = ['User_X', 'User_Y']\n",
    "    # Reorder columns\n",
    "    new_columns = target_columns + [col for col in df.columns if col not in target_columns]\n",
    "    df = df[new_columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "for file in os.listdir(dir):\n",
    "    if file.endswith('.pkl'):\n",
    "        df = process_data(dir+file)\n",
    "        ds.read_data(df)\n",
    "        \n",
    "dataframes = ds.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectories: 368\n"
     ]
    }
   ],
   "source": [
    "for i, df in enumerate(dataframes):\n",
    "    df['time_idx'] = range(len(df))\n",
    "    df['series_id'] = i \n",
    "\n",
    "random.shuffle(dataframes)\n",
    "print(f\"Trajectories: {len(dataframes)}\")\n",
    "all_trajectories = len(dataframes)\n",
    "# Concatenate all dataframes into one\n",
    "full_df = pd.concat(dataframes).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "\n",
    "mean_ = full_df.mean()\n",
    "std_ = full_df.std()\n",
    "\n",
    "for column in full_df.columns:\n",
    "    if column in ['series_id', 'time_idx', 'Gazing_station', 'closest_station']:\n",
    "        continue\n",
    "    full_df[column] = (full_df[column] - full_df[column].mean()) / full_df[column].std()\n",
    "\n",
    "min_ = full_df.min()\n",
    "max_ = full_df.max()\n",
    "\n",
    "for column in full_df.columns:\n",
    "    if column in ['series_id', 'time_idx', 'Gazing_station', 'closest_station']:\n",
    "        continue\n",
    "    full_df[column] = (full_df[column] - full_df[column].min()) / (full_df[column].max() - full_df[column].min())\n",
    "\n",
    "\n",
    "normalize_dict = {'mean': mean_, 'std': std_, 'min': min_, 'max': max_}\n",
    "\n",
    "\n",
    "# def revert function\n",
    "def revert_normalize(df):\n",
    "    global normalize_dict\n",
    "    for column in df.columns:\n",
    "        if column in ['series_id', 'time_idx', 'Gazing_station', 'closest_station']:\n",
    "            continue\n",
    "        df[column] = df[column] * (normalize_dict['max'][column] - normalize_dict['min'][column]) + normalize_dict['min'][column]\n",
    "        df[column] = df[column] * normalize_dict['std'][column] + normalize_dict['mean'][column]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "\n",
    "max_encoder_length = 32\n",
    "max_prediction_length = 20\n",
    "frac = 0.8\n",
    "target = ['User_X', 'User_Y']\n",
    "\n",
    "\n",
    "time_varying_known_reals=['AGV_speed_X', 'AGV_speed_Y', 'AGV_speed', \n",
    "   'User_speed', 'User_velocity_X', 'User_velocity_Y', 'Wait_time',\n",
    "   'start_station_X', 'start_station_Y', 'end_station_X', 'end_station_Y',\n",
    "   'AGV_X', 'AGV_Y', 'rolling_avg'\n",
    "   ]\n",
    "\n",
    "time_varying_unknown_reals=['AGV_distance_X', 'AGV_distance_Y', \n",
    "    'User_speed_X', 'User_speed_Y', 'distance_to_closest_station',\n",
    "    'distance_to_closest_station_X', 'distance_to_closest_station_Y',\n",
    "    'distance_from_start_station_X', 'distance_from_start_station_Y',\n",
    "    'distance_from_end_station_X', 'distance_from_end_station_Y', \n",
    "    'GazeDirection_X', 'GazeDirection_Y', 'GazeDirection_Z', \n",
    "    'Gazing_station', 'closest_station'\n",
    "    ]\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    full_df[lambda x: x.series_id <= all_trajectories * frac],  # Adjust based on your timeline\n",
    "    time_idx=\"time_idx\",\n",
    "    target=target,\n",
    "    group_ids=[\"series_id\"],\n",
    "    min_encoder_length=max_encoder_length,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=max_prediction_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_known_reals= time_varying_known_reals,\n",
    "    time_varying_unknown_reals= time_varying_unknown_reals,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet(\n",
    "    full_df[lambda x: x.series_id > all_trajectories * frac],  # Adjust based on your timeline\n",
    "    time_idx=\"time_idx\",\n",
    "    target=target,\n",
    "    group_ids=[\"series_id\"],\n",
    "    min_encoder_length=max_encoder_length,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=max_prediction_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_known_reals= time_varying_known_reals,\n",
    "    time_varying_unknown_reals= time_varying_unknown_reals,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16  # Adjust based on your GPU capacity\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaoze/anaconda3/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/shaoze/anaconda3/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A2000 8GB Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | MultiLoss                       | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0     \n",
      "3  | prescalers                         | ModuleDict                      | 480   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 0     \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 38.0 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 15.3 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K \n",
      "11 | lstm_encoder                       | LSTM                            | 8.4 K \n",
      "12 | lstm_decoder                       | LSTM                            | 8.4 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.2 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K \n",
      "20 | output_layer                       | ModuleList                      | 66    \n",
      "----------------------------------------------------------------------------------------\n",
      "107 K     Trainable params\n",
      "0         Non-trainable params\n",
      "107 K     Total params\n",
      "0.430     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187fe7a52d614b7d9e9cdeb854fb813c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388120e99e8340e4becc34358e8b0406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46adb670617941189f3133df59a9271a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fe3d25f81045ca9d068e816819ae40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc89f827bb534691b334dc8db150bcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b9c07c499047de8ee15e4ebfe53c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_forecasting.metrics import RMSE\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=32,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=[1,1], \n",
    "    loss=nn.MSELoss(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=1, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=20, \n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=TensorBoardLogger(\"../model/TFT_logs\"), \n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
