{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "cur_dir = os.path.dirname(os.path.abspath(\"__file__\"))  # Gets the current notebook directory\n",
    "src_dir = os.path.join(cur_dir, '../')  # Constructs the path to the 'src' directory\n",
    "# Add the 'src' directory to sys.path\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "from src.constant import *\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.MyDataset import MyDataset\n",
    "from src.TraPredModel import TraPredModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/PandasData/Sampled/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[new_columns]\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mdir\u001b[39m):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     36\u001b[0m         df \u001b[38;5;241m=\u001b[39m process_data(\u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m+\u001b[39mfile)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/PandasData/Sampled/'"
     ]
    }
   ],
   "source": [
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "lookback = 20\n",
    "dir = '../data/PandasData/Sampled/'\n",
    "ds = MyDataset(lookback=lookback)\n",
    "\n",
    "def process_data(df_dir : str, target_freq : int = 10):\n",
    "    df: pd.DataFrame = pd.read_pickle(df_dir)\n",
    "    df.dropna(inplace=True, how='any')\n",
    "    f_per_sec = df.groupby('TimestampID').count().mean().mean()\n",
    "    if f_per_sec < target_freq:\n",
    "        raise ValueError('The frequency of the data is lower than the target frequency')\n",
    "    elif int(f_per_sec) == target_freq:\n",
    "        pass\n",
    "    else:\n",
    "        resample_ratio = int(f_per_sec/target_freq)\n",
    "        df = df.iloc[::resample_ratio, :]\n",
    "    # # for origin\n",
    "    for drop_column in ['Confidence', 'Timestamp', 'TimestampID', \n",
    "                          'DatapointID', 'PID', 'SCN', 'U_X', 'U_Y', 'U_Z', \n",
    "                          'AGV_Z', 'User_Z', 'GazeOrigin_Z', 'User_Pitch', 'User_Yaw', 'User_Roll', \n",
    "                          'EyeTarget']:\n",
    "        df = df.drop(columns=[drop_column], errors='ignore')\n",
    "\n",
    "    target_columns = ['User_X', 'User_Y']\n",
    "    # Reorder columns\n",
    "    new_columns = target_columns + [col for col in df.columns if col not in target_columns]\n",
    "    df = df[new_columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "for file in os.listdir(dir):\n",
    "    if file.endswith('.pkl'):\n",
    "        df = process_data(dir+file)\n",
    "        ds.read_data(df)\n",
    "\n",
    "stats_dict = {'mean': 0, 'std': 0, 'min': 0, 'max': 0}\n",
    "stats_dict = ds.normalize_dataset()\n",
    "ds.generate_data()\n",
    "\n",
    "train:torch.utils.data.DataLoader\n",
    "test:torch.utils.data.DataLoader\n",
    "train, test = ds.split_data(frac=0.9, shuffle=True, batch_size=4)\n",
    "\n",
    "\n",
    "feature_dim = ds.feature_dim\n",
    "print(f\"columns : {df.columns} \\nfeature_dim : {feature_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 20, 32]) torch.Size([4, 20, 32])\n",
      "20562 2285\n"
     ]
    }
   ],
   "source": [
    "for i, (X, y) in enumerate(train):\n",
    "    print(X.shape, y.shape)\n",
    "    break\n",
    "\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TraPredModel(\n",
       "  (lstm): LSTM(32, 128, num_layers=20, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TraPredModel(input_size=feature_dim, lookback=lookback, hidden_size=128, bidirectional=True)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight_ih' in name:  # input-hidden weights in LSTM cells\n",
    "        torch.nn.init.xavier_uniform_(param.data)\n",
    "    elif 'weight_hh' in name:  # hidden-hidden weights\n",
    "        torch.nn.init.orthogonal_(param.data)\n",
    "    elif 'bias' in name:  # biases\n",
    "        param.data.fill_(0)\n",
    "    elif 'weight' in name:  # linear layers weights\n",
    "        if param.dim() >= 2:  # Only apply to weights with 2 or more dimensions\n",
    "            torch.nn.init.kaiming_normal_(param.data)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device}\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f8cace785848a78c8da09de5ec7933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "eval_step = 3000\n",
    "# model = TraPredModel(input_size=numeric_df.shape[1], lookback=lookback)\n",
    "\n",
    "train_all = len(train)\n",
    "\n",
    "loss_all = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for step, (X_batch, y_batch) in tqdm(enumerate(train), total = train_all):\n",
    "        X_batch = X_batch.float().to(device)\n",
    "        y_batch = y_batch.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(X_batch)\n",
    "        loss = torch.mean(loss_fn(y_pred, y_batch[:, :1, :].squeeze(1)))\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Loss is NaN\")\n",
    "            continue\n",
    "        loss_all.append(loss.item())\n",
    "        loss.backward()\n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        if (epoch * train_all + step + 1) % eval_step == 0:\n",
    "            print(f\"Start testing\")\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                all_test = len(test)\n",
    "                test_rmse_all = []\n",
    "                for X_test_batch, y_test_batch in tqdm(test):\n",
    "                    X_test_batch = X_test_batch.float().to(device)\n",
    "                    y_test_batch = y_test_batch.float().to(device)\n",
    "                    y_pred = model(X_test_batch)\n",
    "                    test_rmse = torch.mean(loss_fn(y_pred, y_test_batch[:, :1, :]))\n",
    "                    test_rmse = torch.sqrt(test_rmse)\n",
    "                    if not torch.isnan(test_rmse):\n",
    "                        test_rmse_all.append(test_rmse.item())\n",
    "\n",
    "                print(\"Epoch %d: test RMSE %.4f\" % (epoch+1, sum(test_rmse_all)/all_test))\n",
    "            \n",
    "            model.train()\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_np = np.sqrt(np.array(loss_all))\n",
    "np.save('../model/loss_baseline.npy', loss_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
